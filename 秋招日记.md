# 秋招日记

[toc]



主投 Java

## 9.10 美团 AI 面

复习 Java 基础

1. JIT 保存热点代码对应的机器码
2. AOT 直接运行前将字节码转换为机器码
3. checked Exception 和 unchecked Exception: 受检异常需要被处理才能通过编译，unchecked Exception 如 RuntimeException 为**不受检查异常**，不处理也可以通过编译。
4. BIO 阻塞 NIO 非阻塞 AIO 异步
5. HashMap 和 HashTable 的区别
   1. HashMap 非线程安全，HashTable 线程安全
   2. HashMap 可以存 null 的键值，HashTable 则不行
   3. 默认大小和扩容机制略有不同
   4. HashMap 在链表长度大于 8 时转换为红黑树
   5. HashMap 的哈希值计算更复杂(**对原始hashCode进行扰动)**，冲突更少，HashTable 直接使用 hashCode()
6.  ConcurrentHashMap 相关
   1. 1.7 的实现锁 segment，并发度低，1.8 之后 synchronize + cas 锁 Node，并发度高。
   2. concurrentHashMap 的 kv 不能是 null
   3. size() 方法：sumCount() 方法拿 baseCount，再加上补偿 CounterCells（addCount 里 baseCount CAS 失败时的补偿，用于分布加法，类似于 CRDT ？）
7. Java 内存区域
   1. 本地内存：方法区 + 直接内存
   2. 堆（GC堆）
      1. 新生代 Elden + S0 + S1 老年代 + 元空间
   3. 线程私有：本地方法栈、虚拟机栈、程序计数器

8. 内存分配原则

   1. 优先分配在 Eden，没有空间时 minor GC，若 minor GC 后还是没有空间，分配到老年代
   2. **分配担保机制**：确保老年代能容纳所有新生代对象
   3. 大的对象直接到老年代
   4. 15 or 动态年龄次存货对象进入老年代

9. 可达性分析用于判断可回收对象。从栈对象、静态对象等 GC Roots 出发搜索引用链。对象没有被引用链相连说明可以回收。

10. 引用类型

    1. 强引用
    2. 软引用：对象只有软引用时，GC可能被回收。可以用于缓存，可以搭配引用队列
    3. 弱引用：对象只有弱引用时，GC时会被回收。可以用于缓存、对象池、代理对象避免循环引用。可以搭配引用队列。
    4. 虚引用：主要用于跟踪对象被垃圾回收的活动。

11. GC算法

    1. 标记-清除
    2. 标记-复制
    3. 标记-整理
    4. 分代收集：新生代标记复制，老年代标记整理

12. GC 器

    1. Serial 串行收集器

    2. ParNew 并行收集器

    3. Parallel Scavenge 收集器，JDK 1.8 默认，关注吞吐量。

    4. Serial Old

    5. Parallel Old

    6. CMS Concurrent **Mark** Sweep 追求最短回收时间的并发收集器，使用标记-清除

       1. 初始标记 STW
       2. 可达性分析并发标记
       3. 重新标记 STW
       4. 并发清除

    7. G1 收集器：进行 Mixed GC，Region 中标记-整理，之间标记-复制

       1. 把堆分成 Region，包含 Eden、survivor、老年区等

       2. Region 可以按照回收价值排序，用于预估时间和空间

       3. Remember Set 用于记录引用集合，降低可达性分析成本

       4. 过程

          1. 初试标记，记录root
          2. 并发标记，可达性分析
          3. 最终标记，合并 Remember Set + log
          4. 并发筛选回收：排序 + 回收 + 复制            

          ![G1 收集器](https://oss.javaguide.cn/github/javaguide/java/jvm/g1-garbage-collector.png)





Linux 常用命令

#### 文件权限管理

ls -l 可以看权限，文件类型(d/-/l) + 所有者权限u + 组权限g + 其他用户权限o，root 用户无视权限。

![img](https://javaguide.cn/assets/Linux%E6%9D%83%E9%99%90%E8%A7%A3%E8%AF%BB-BGnXOuG0.png)

* chown 更改属主
* chgrp 更改文件所在组
* chmod 修改文件权限
  * 字母法 chmod g+x
  * 数字法 chmod 700

#### 用户管理

* useradd
* userdel
* usermod
* passwd  设置认证信息
* su switch user
* groupadd
* groupdel
* groupmod

#### 系统状态

Kafka 复习

![img](https://oss.javaguide.cn/github/javaguide/high-performance/message-queue20210507200944439.png)

基本概念

* 生产者
* 消费者
* broker 代理：相当于 kafka 实例
* topic 主题
* partition 分区
* Replica 副本

#### 顺序消费

消息分发到 partition 时分配偏移量、partition 中消息有序

保证顺序的方式

1. 1 个 topic 只有一个 partition
2. 发送消息时指定 key / partition，同一个 key 的消息发到同一个 partition

#### 消息不丢失

**生产者丢失**

生产者异步发送消息，可以调用 get 阻塞等待结果，也可以添加回调函数处理发送结果。

发送失败重试，默认为 3。

**消费者丢失消息**

默认：消费者拉取到消息后，commit offset，但是可能宕机，没有实际消费。

解决：手动 commit。（宕机会重复消费消息）

**kafka 丢失消息**

发送的消息被成功复制到 k 个 Replica 的 ACK 才算成功。

#### 消息不重复

幂等校验



#### 重试机制

消费失败时，会进行重试。多次仍不行则跳过，继续后面的消息的消费

* 默认重试 10 次
* 重试间隔为 0

可以自定义重试次数重试间隔和错误处理

#### 死信队列

处理失败的消息, 达到最大重试次数后, 进入死信队列

`@RetryableTopic` 是 Spring Kafka 中的一个注解,它用于配置某个 Topic 支持消息重试，更推荐使用这个注解来完成重试。

可以设置死信处理

或者重新消费



### RocketMQ

类似于 kafka，在架构上做减法，在功能上做加法

简化架构

* zookeeper -> nameserver
* partition -> queue
* queue 里只存 offset，消息实体放在 commit log 上
* 多 Topic 场景下写性能好。
  * kafka 写 partition 是写到 segment 里，**随机写**多个segment。
  * 多个 Topic 的消息放在“一个”逻辑 commit Log 里，顺序写性能好 

* 简化备份
  * kafka 主从 partition 之间需要通信，同步 segment。区分主从partition
  * 直接同步 commit log 文件。区分主从 broker

功能加法

* 消息过滤
  * kafka 只能按 topic 区分
  * rocketMq 支持打 tag
* 事务
* 延时消息
  * 延时队列
  * kafka 需要自己实现
* 死信队列
  * kafka 需要自己实现死信
  * rocketMQ 原生支持
* 消息回溯
  * kafka 调整offset
  * rocket 调整 offset + time

性能不如kafka

## 9.11 字节跳动国际电商

复习:

#### 进程间通信

* 管道
* 消息队列
* 信号量
* 信号
* 共享内存
* socket

#### 死锁

四个条件

* 互斥
* 请求和保持
* 不可剥夺
* 循环等待

排查

* pstack
* gdb info thread + bt
  * s 单步
  * n 单步跳过
  * p 查看变量
* jstack

避免死锁

有序分配



### 复习数据库

1. 行记录怎么存的: 表 ->  段 -> 区 -> 页16K -> 行
2. 索引覆盖: 索引中包含索要查找的信息, 如使用联合索引, 或查找的就是 id
3. 索引下推: 索引 -> 回表 -> 过滤 ===> 索引 -> 过滤 -> 回表. 比如联合索引, 索引第一个 key 的同时, 过滤第二个 key
4. 聚簇索引(主键索引) 叶子节点存放真实数据, 非聚簇索引存放主键
5. 







## 9.12 CDG 腾讯广告 一面

#### 分布式锁

1. 数据库, 悲观锁 & 乐观锁
2. redis Lua (setnx + expire) 释放 Lua （判断，删除）
3. zookeeper





字节国际电商一面 50min

【实习：活动开发怎么做的，rocks DB、分布式锁为什么lua脚本】

kafka 的优势，为什么用 kafka，怎么有序消费

为什么 B+ 树，为什么 B+ 树树高比较低？

蟹行锁是什么？乐观上锁是什么？

锁怎么管理的？

隔离级别怎么做的

ES 用过吗

IO 模型，为什么 IO 多路复用，解决什么问题

为什么用跳表

做题 5min：

链表倒数第 k 个节点

反问：业务ticktok 电商支付模块，达人分佣。几面：两轮技术、sp三轮。语言：go。



9.12 cdg 腾讯广告

#### ES 小结

一个 segment 包括

* 倒排索引 term dic + freq
* term Index 字典树、自动机（前后缀匹配）
* stored fields 行存储
* doc values 列存储

Lucene 包含多个 segment，是一个单机搜索库

* segment 生成后不再修改，负责读

* 新的文档写入生成新的segment
* 并发读 segment
* 定期合并 segment

ES 拓展了高性能、高扩展

* 文档分成多个 shards，提高扩展性，每个分片是一个 Lucene
* 高扩展：分多个 Node
* 高可用：分片分成主分片和副本分片
* 区分 Node 类型：Master、Data、协调节点
* 去中心化，集成 raft

ES流程

* 写
  * 协调节点找DataNode 和 shard，写 Lucene。
  * 主分片复制给副本分片，ACK
* 搜索
  * query Phase：聚合分片搜索结果，拿到 doc id
    * 协调节点将请求分发到多个 shard
    *  Lucene 并发搜索多个 segment，倒排索引拿 id，doc value 排序
    * 分片将结果返回协调节点，协调节点聚合后拿到 doc id
  * fetch Phase：根据 doc id 拿结果
    * 协调节点用 doc id 请求分片库
    * Lucene 从每个segment 的 Stroed filelds（行存）读出完整的文档内容



#### Kafka 为什么这么快

零拷贝技术 (0 CPU拷贝)

* mmap 将内核空间的缓冲区映射到用户空间，剩下一次内核空间到用户空间的拷贝
* send file 从内核缓冲区直接拷贝到网卡（跳过socket 网卡 SGDMA）

RocketMq 使用 mmap

Kafka 用的 sendfile 拷贝次数和空间切换次数少

mmap可以处理消息，sendfile 直接返回



Kafka 和 RokcetMq 怎么选

大数据场景 kafka

其他场景尽量 rocket mq



#### IO 模型

1. **阻塞 IO**：线程发起一个 IO 操作，等待直到完成

2. **非阻塞 IO**：线程等待一个 IO 操作，然后轮询检查 IO 操作

3. **IO 多路复用**：线程同时等待多个 IO 事件：通过 select、poll、epoll 等系统调用，当某个 IO 完成，应用会被通知。适合**高并发**。

4. **异步**：发起 IO 后立即做其他事情。IO 完成时收到通知。Linux 低版本原生不支持。

5. **信号驱动**：发起 IO 后立即做其他事情。IO 完成时收到信号。



多路复用

为每个请求分配一个线程不合适

* select
  * 已连接的 Socket 放在集合里，交给内核
  * 内核遍历集合，表记 可读/ 可写事件，拷贝回用户态
  * 用户遍历集合，处理读写事件
  * 使用**固定长度的 BitsMap**，最大 1024

* poll
  * 改用**动态数组**存储文件描述符，用链表形式组织

* epoll
  * epoll_create 创建 epfd + epoll_ctl 添加 fd + epoll_wait 返回事件个数
  * 用红黑树跟踪 fd，存储在内核中
  * **事件驱动**，socket 有事件时，内核回调函数将 socket 加入**就绪链表**
  * 边缘触发和水平触发
    * 边缘触发：有事件的时候 epoll wait **苏醒一次**，需要一次读完 read 缓冲区。
    * 水平触发：有事件就**不断苏醒**
* 多路复用一般搭配非阻塞 IO，否则会在调用 read/write 时程序阻塞。





## 9.13 美团核心本地商业-基础研发平台 后端一面

1. Spring 循环依赖，sping 使用了三级缓存的设计，解决单例模式带来的循环依赖问题。
   1. Bean 的生命周期
      1. 实例化 instantiation
      2. 属性赋值 populate
      3. 初始化 initialization
      4. 销毁 destroy
   2. 第一次使用 bean 时会创建 bean 对象，放入 IoC 容器的缓存中，后续使用会从缓存池获取。
      1. 一级缓存 singletonObjects：存放初始化好的 bean
      2. 二级缓存 earlySingletonObjects：存放原始的**未填充属性**的 bean
      3. 三级缓存 singletonFactories：存放 bean 工厂对象，用来**解决循环依赖**
      4. getSingleton() 方法
         1. 先尝试从一级缓存拿成品
         2. 如果拿不到且在建立中，尝试从二级缓存获取
         3. 如果仍然拿不到且允许从三级缓存拿，则从三级缓存的工厂获取，如果拿到了，放入二级缓存
      5. 三级缓存的 ObejctFactory.getObject() 可以解决循环依赖
         1. bean 被实例化（构造函数）后，其引用 (getEarlyBeanReference) 会被加入三级缓存
         2. 若 AB 相互依赖
            1. A 实例化，放入三级缓存等待 B 创建
            2. B 实例化，从三级缓存拿到 A 赋值属性，然后放入一级缓存
            3. A 从一级缓存拿到 B，完成创建
      6. Spring 不能解决构造循环依赖，因为无法完成实例化，放入三级缓存
      7. 不能解决 prototype 循环依赖
      8. 不能解决多例的循环依赖，因为每次 getBean 都会创建一个新的对象，**不会被缓存**
         1. 可以用 @Lazy 注解延迟加载
      9. 为什么用三级缓存？
         1. 如果 Spring 选择二级缓存来解决循环依赖的话，那么就意味着所有 Bean 都需要在实例化完成之后就立马为其创建代理，而 Spring 的设计原则是在 Bean 初始化完成之后才为其创建代理。
      10. 为什么要第二级缓存？间接循环依赖 A->B->C->A 回到A的时候，B已经有二级缓存了，可以完成创建。



Java 常见的锁

* 公平锁按申请顺序获得
* 非公平锁，吞吐量大
  * 在入队之前之前 CAS 尝试获得锁
* 可重入锁：可以递归调用，同一线程多次获取
  * state ++ ： ReentrantLock和synchronized都是可重入锁
* 独占锁、共享锁
  * 独占锁只能被一个线程获得
  * 共享锁可以被多个线程获得
* 读写锁，也是互斥锁、共享锁的一种
* 悲观锁、乐观锁



## 中秋摆了。。。

## 9.18 阿里国际笔试

笔试做的有点烂，应该是挂了

面经学习

1. **CAS 问题**
   1. ABA：线程 1 和 2 希望将金额从 200 减为 100，线程 1 成功了，此时线程 3 将金额从 100 增加为 200 (存款)，线程 2 再次执行取款操作。出现问题。解决：版本号
   2. 单变量
   3. 忙等待
2. Redis 补充：**Redis 事务**
   1. Redis 事务
      1. MULTI 开启事务，将命令逐个放入队列
      2. EXEC 执行事务中的操作
      3. DISCARD 取消事务，放弃执行事务块中的命令
      4. WATCH 监视 keys，若事务执行前 keys 改变，不执行事务
      5. UNWATCH
   2. Redis 事务理解
      1. Redis 事务失败时继续执行之后的命令
      2. Redis 事务中的命令：要么全部执行，要么全部不执行
      3. 也可以基于 Lua 脚本实现 Redis 事务，区别不大
   3. **集群**环境分布式锁
      1. redlock 算法：客户端和多个独立的 Redis 节点依次请求申请加锁，如果客户端能够和**半数以上的节点成功地完成加锁操作**，那么我们就认为，客户端成功地获得分布式锁，否则加锁失败。
3. Redis 补充：**复制**
   1. 主从复制，读写分离
   2. 全量复制、基于长连接的命令传播、增量复制
4. Redis 补充：**哨兵**
   1. 负责监控、选主、通知



## 9.20 饿了么一面 美团二面

##### Java 并发容器

基本都在 java.util.concurrent 包中

1. ConcurrentHashMap 线程安全的 HashMap
2. **`ConcurrentLinkedQueue`** 非阻塞的并发队列
3. BlockingQueue 阻塞队列
4. **`ConcurrentSkipListMap`**
5. **`CopyOnWriteArrayList`** 线程安全的 List



##### RAFT 读优化

**强一致性读**：t1 的时间我们写入了一个值，那么在 t1 之后，我们的读一定能读到这个值，不可能读到 t1 之前的值。

-> **核心问题**：确认 leader 在处理这次 read 的时候一定是 leader 

LogRead

走一次 Raft Log，Apply 的时候处理 read。



ReadIndex

Leader 处理读请求的时候：

1. 记录当前的 commit index

2. 发送一次 heartBeat 以确认自己是 Leader （而不是通过 log 来确认是否为 Leader）
3. 等待状态机 Apply 到 index

实现follower read

follower 收到 read 请求之后，直接给 leader 发送一个获取 ReadIndex 的命令，leader 仍然走一遍之前的流程，然后将 ReadIndex 返回给 follower，follower 等到当前的状态机的 apply index 超过 ReadIndex 之后，就可以 read 然后将结果返回给 client 了。



Lease Read



其他优化：

batch 读写





日志匹配加速：

follwer 可能同一位置有其他任期的 log，旧的任期的log，没有log等，一条一条恢复很慢。

可以 AppendEntries 消息的回复中，额外携带三个数据：

1. XTerm 冲突的任期
2. XIndex 对应任期的第一条 log
3. XLen 空白的 log 数

```c++
假定Leader 要发一条任期为 6 的log
1. 情况1 同位置有其他任期的 log
Follower	4 5 5 5
Leader		4 6 6 6 6
返回XTerm=5，XIndex=2。Leader如果没有 XTerm 的log，回退到 XIndex
2. 情况2 同位置有旧任期的 log
Follower	4 4 4
Leader		4 6 6 6 6
返回XTerm=4，XIndex=1。Leader发现有 XTerm 的log，回退到下一个任期的 index
3. 情况3 
Follwer		4
Leader		4 6 6 6 6
返回XTerm=-1, XLen = 2。Leader发现 Follower 缺少这个任期的log，回退到 XLen
```



